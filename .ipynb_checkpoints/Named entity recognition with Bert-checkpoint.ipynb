{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fa4b0d9-b5bb-4fac-bcc4-fb28f8a27876",
   "metadata": {},
   "source": [
    "# Named entity recognition with Bert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb589185-65b9-40c7-acdf-db58d03b3de2",
   "metadata": {},
   "source": [
    "This is my NER series notebook projects. I will show you how you we finetune the Bert model to do state-of-the art named entity recognition. First lets install the amazing transformers package by huggingface with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46a9a947-2a5f-4bd1-a1d7-c9b5b08a2d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==2.6.0\n",
      "  Downloading transformers-2.6.0-py3-none-any.whl (540 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m540.9/540.9 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/ashleshkhajbage/opt/anaconda3/lib/python3.9/site-packages (from transformers==2.6.0) (1.23.5)\n",
      "Collecting tokenizers==0.5.2 (from transformers==2.6.0)\n",
      "  Downloading tokenizers-0.5.2.tar.gz (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.6/64.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: boto3 in /Users/ashleshkhajbage/opt/anaconda3/lib/python3.9/site-packages (from transformers==2.6.0) (1.24.28)\n",
      "Requirement already satisfied: filelock in /Users/ashleshkhajbage/opt/anaconda3/lib/python3.9/site-packages (from transformers==2.6.0) (3.6.0)\n",
      "Requirement already satisfied: requests in /Users/ashleshkhajbage/opt/anaconda3/lib/python3.9/site-packages (from transformers==2.6.0) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/ashleshkhajbage/opt/anaconda3/lib/python3.9/site-packages (from transformers==2.6.0) (4.64.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/ashleshkhajbage/opt/anaconda3/lib/python3.9/site-packages (from transformers==2.6.0) (2022.7.9)\n",
      "Collecting sentencepiece (from transformers==2.6.0)\n",
      "  Downloading sentencepiece-0.2.0-cp39-cp39-macosx_10_9_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting sacremoses (from transformers==2.6.0)\n",
      "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: botocore<1.28.0,>=1.27.28 in /Users/ashleshkhajbage/opt/anaconda3/lib/python3.9/site-packages (from boto3->transformers==2.6.0) (1.27.28)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /Users/ashleshkhajbage/opt/anaconda3/lib/python3.9/site-packages (from boto3->transformers==2.6.0) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /Users/ashleshkhajbage/opt/anaconda3/lib/python3.9/site-packages (from boto3->transformers==2.6.0) (0.6.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/ashleshkhajbage/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers==2.6.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ashleshkhajbage/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers==2.6.0) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ashleshkhajbage/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers==2.6.0) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ashleshkhajbage/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers==2.6.0) (2022.9.24)\n",
      "Requirement already satisfied: click in /Users/ashleshkhajbage/opt/anaconda3/lib/python3.9/site-packages (from sacremoses->transformers==2.6.0) (8.0.4)\n",
      "Requirement already satisfied: joblib in /Users/ashleshkhajbage/opt/anaconda3/lib/python3.9/site-packages (from sacremoses->transformers==2.6.0) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/ashleshkhajbage/opt/anaconda3/lib/python3.9/site-packages (from botocore<1.28.0,>=1.27.28->boto3->transformers==2.6.0) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ashleshkhajbage/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.28.0,>=1.27.28->boto3->transformers==2.6.0) (1.16.0)\n",
      "Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sentencepiece-0.2.0-cp39-cp39-macosx_10_9_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: tokenizers\n",
      "  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[46 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-cpython-39\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-cpython-39/tokenizers\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-39/tokenizers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-cpython-39/tokenizers/models\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/models/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-39/tokenizers/models\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-cpython-39/tokenizers/decoders\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/decoders/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-39/tokenizers/decoders\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-cpython-39/tokenizers/normalizers\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/normalizers/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-39/tokenizers/normalizers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-cpython-39/tokenizers/pre_tokenizers\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/pre_tokenizers/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-39/tokenizers/pre_tokenizers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-cpython-39/tokenizers/processors\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/processors/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-39/tokenizers/processors\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-cpython-39/tokenizers/trainers\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/trainers/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-39/tokenizers/trainers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-cpython-39/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/implementations/byte_level_bpe.py -> build/lib.macosx-10.9-x86_64-cpython-39/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/implementations/sentencepiece_bpe.py -> build/lib.macosx-10.9-x86_64-cpython-39/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/implementations/base_tokenizer.py -> build/lib.macosx-10.9-x86_64-cpython-39/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/implementations/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-39/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/implementations/char_level_bpe.py -> build/lib.macosx-10.9-x86_64-cpython-39/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/implementations/bert_wordpiece.py -> build/lib.macosx-10.9-x86_64-cpython-39/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/__init__.pyi -> build/lib.macosx-10.9-x86_64-cpython-39/tokenizers\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/models/__init__.pyi -> build/lib.macosx-10.9-x86_64-cpython-39/tokenizers/models\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/decoders/__init__.pyi -> build/lib.macosx-10.9-x86_64-cpython-39/tokenizers/decoders\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/normalizers/__init__.pyi -> build/lib.macosx-10.9-x86_64-cpython-39/tokenizers/normalizers\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/pre_tokenizers/__init__.pyi -> build/lib.macosx-10.9-x86_64-cpython-39/tokenizers/pre_tokenizers\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/processors/__init__.pyi -> build/lib.macosx-10.9-x86_64-cpython-39/tokenizers/processors\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/trainers/__init__.pyi -> build/lib.macosx-10.9-x86_64-cpython-39/tokenizers/trainers\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m running build_rust\n",
      "  \u001b[31m   \u001b[0m error: can't find Rust compiler\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m To update pip, run:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m     pip install --upgrade pip\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m and then retry package installation.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25hFailed to build tokenizers\n",
      "\u001b[31mERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers==2.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7abe83d-f94e-4969-848d-04b0a74c12eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/ashleshkhajbage/opt/anaconda3/lib/python3.9/site-packages (24.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc748d6-002f-4f20-a6bb-9ec28e95cda6",
   "metadata": {},
   "source": [
    "\n",
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eacbd618-3aaf-4d60-9eb3-f47e223c93ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1048565</th>\n",
       "      <td>Sentence: 47958</td>\n",
       "      <td>impact</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048566</th>\n",
       "      <td>Sentence: 47958</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048567</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>Indian</td>\n",
       "      <td>JJ</td>\n",
       "      <td>B-gpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048568</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>forces</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048569</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>said</td>\n",
       "      <td>VBD</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048570</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>they</td>\n",
       "      <td>PRP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048571</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>responded</td>\n",
       "      <td>VBD</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048572</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048573</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048574</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>attack</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Sentence #       Word  POS    Tag\n",
       "1048565  Sentence: 47958     impact   NN      O\n",
       "1048566  Sentence: 47958          .    .      O\n",
       "1048567  Sentence: 47959     Indian   JJ  B-gpe\n",
       "1048568  Sentence: 47959     forces  NNS      O\n",
       "1048569  Sentence: 47959       said  VBD      O\n",
       "1048570  Sentence: 47959       they  PRP      O\n",
       "1048571  Sentence: 47959  responded  VBD      O\n",
       "1048572  Sentence: 47959         to   TO      O\n",
       "1048573  Sentence: 47959        the   DT      O\n",
       "1048574  Sentence: 47959     attack   NN      O"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "data = pd.read_csv(\"ner_dataset.csv\", encoding=\"latin1\").fillna(method=\"ffill\")\n",
    "data.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a20a5a89-c6bf-46b8-8892-4168cbf46046",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n",
    "                                                           s[\"POS\"].values.tolist(),\n",
    "                                                           s[\"Tag\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "\n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5611c071-c291-4e5a-bc12-6bfab4b77dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "getter = SentenceGetter(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfe3cf8b-6197-4bc1-8b15-9109aa95d4cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thousands',\n",
       " 'of',\n",
       " 'demonstrators',\n",
       " 'have',\n",
       " 'marched',\n",
       " 'through',\n",
       " 'London',\n",
       " 'to',\n",
       " 'protest',\n",
       " 'the',\n",
       " 'war',\n",
       " 'in',\n",
       " 'Iraq',\n",
       " 'and',\n",
       " 'demand',\n",
       " 'the',\n",
       " 'withdrawal',\n",
       " 'of',\n",
       " 'British',\n",
       " 'troops',\n",
       " 'from',\n",
       " 'that',\n",
       " 'country',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [[word[0] for word in sentence] for sentence in getter.sentences]\n",
    "\n",
    "sentences[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "780de8c4-4d41-4d6a-afc8-1ac2c1a25b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "labels = [[s[2] for s in sentence] for sentence in getter.sentences]\n",
    "\n",
    "print(labels[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e57de7-7ea9-44b7-931b-14fd72b2ef4e",
   "metadata": {},
   "source": [
    "### Apply BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "600ce4ca-6ace-4bfc-b067-7ba22f2cffab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.38.1-py3-none-any.whl.metadata (131 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.1/131.1 kB\u001b[0m \u001b[31m460.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /Users/ashleshkhajbage/opt/anaconda3/lib/python3.9/site-packages (from transformers) (3.6.0)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers)\n",
      "  Downloading huggingface_hub-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/ashleshkhajbage/opt/anaconda3/lib/python3.9/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ashleshkhajbage/opt/anaconda3/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/ashleshkhajbage/opt/anaconda3/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/ashleshkhajbage/opt/anaconda3/lib/python3.9/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /Users/ashleshkhajbage/opt/anaconda3/lib/python3.9/site-packages (from transformers) (2.28.1)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Downloading tokenizers-0.15.2-cp39-cp39-macosx_10_12_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.2-cp39-cp39-macosx_10_12_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/ashleshkhajbage/opt/anaconda3/lib/python3.9/site-packages (from transformers) (4.64.1)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.19.3->transformers)\n",
      "  Downloading fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/ashleshkhajbage/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/ashleshkhajbage/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/ashleshkhajbage/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ashleshkhajbage/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ashleshkhajbage/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ashleshkhajbage/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2022.9.24)\n",
      "Downloading transformers-4.38.1-py3-none-any.whl (8.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.20.3-py3-none-any.whl (330 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.1/330.1 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.2-cp39-cp39-macosx_10_12_x86_64.whl (426 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.4/426.4 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.15.2-cp39-cp39-macosx_10_12_x86_64.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.9/170.9 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, fsspec, huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2022.7.1\n",
      "    Uninstalling fsspec-2022.7.1:\n",
      "      Successfully uninstalled fsspec-2022.7.1\n",
      "Successfully installed fsspec-2024.2.0 huggingface-hub-0.20.3 safetensors-0.4.2 tokenizers-0.15.2 transformers-4.38.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47bb5cd-4902-484c-a40b-7800e655bc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "torch.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81b3c91-2d52-4acb-9183-e7b1152c36ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 75\n",
    "bs = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a17ea10-6689-46cd-b702-867412d2399d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb39114-069e-4576-aafe-85e62a127ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.get_device_name(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce512650-31a0-4f09-ab76-68d26232042a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
